{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40178,
     "status": "ok",
     "timestamp": 1588213047201,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "rPwL9bdoBNzQ",
    "outputId": "553f83f0-cbf1-48d5-a184-4f4c8ff055ac"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import PIL\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import timm\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import logging\n",
    "import fnmatch\n",
    "import argparse\n",
    "import torchvision\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from timm.models.layers.activations import *\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from collections import OrderedDict, defaultdict\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from randaugment import RandAugment, ImageNetPolicy, Cutout\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 179460,
     "status": "ok",
     "timestamp": 1588213186502,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "yyGpxuktB96O",
    "outputId": "584ea32f-dbe1-4465-8e60-e0f4e5c96a6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID-19', 'NonCOVID-19']\n",
      "{'train': 639, 'val': 75}\n",
      "cuda:0\n",
      "{0: 'COVID-19', 1: 'NonCOVID-19'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([46, 3, 224, 224])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_dir = '/home/linh/Downloads/Covid-19/CT/Soares'\n",
    "\n",
    "data_dir = '/home/linh/Downloads/Covid-19/CT/Yang'\n",
    "#data_dir = '/home/linh/Downloads/Covid-19/CT/Moh'\n",
    "\n",
    "# Define your transforms for the training and testing sets\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        RandAugment(),\n",
    "        ImageNetPolicy(),\n",
    "        Cutout(size=16),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Load the datasets with ImageFolder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "batch_size = 46\n",
    "data_loader = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=8, pin_memory = True)\n",
    "              for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "print(class_names)\n",
    "print(dataset_sizes)\n",
    "print(device)\n",
    "\n",
    "### we get the class_to_index in the data_Set but what we really need is the cat_to_names  so we will create\n",
    "_ = image_datasets['train'].class_to_idx\n",
    "cat_to_name = {_[i]: i for i in list(_.keys())}\n",
    "print(cat_to_name)\n",
    "    \n",
    "# Run this to test the data loader\n",
    "images, labels = next(iter(data_loader['val']))\n",
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226470,
     "status": "ok",
     "timestamp": 1588213233519,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "N350JAHpu8c3",
    "outputId": "96a2d095-f78f-4ca5-eb0c-c5390e367831"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def showimage(data_loader, number_images, cat_to_name):\\n    dataiter = iter(data_loader)\\n    images, labels = dataiter.next()\\n    images = images.numpy() # convert images to numpy for display\\n    # plot the images in the batch, along with the corresponding labels\\n    fig = plt.figure(figsize=(number_images, 4))\\n    for idx in np.arange(number_images):\\n        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\\n        img = np.transpose(images[idx])\\n        plt.imshow(img)\\n        ax.set_title(cat_to_name[labels.tolist()[idx]])\\n        \\n#### to show some  images\\nshowimage(data_loader['test'], 20, cat_to_name)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def showimage(data_loader, number_images, cat_to_name):\n",
    "    dataiter = iter(data_loader)\n",
    "    images, labels = dataiter.next()\n",
    "    images = images.numpy() # convert images to numpy for display\n",
    "    # plot the images in the batch, along with the corresponding labels\n",
    "    fig = plt.figure(figsize=(number_images, 4))\n",
    "    for idx in np.arange(number_images):\n",
    "        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\n",
    "        img = np.transpose(images[idx])\n",
    "        plt.imshow(img)\n",
    "        ax.set_title(cat_to_name[labels.tolist()[idx]])\n",
    "        \n",
    "#### to show some  images\n",
    "showimage(data_loader['test'], 20, cat_to_name)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226461,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "L9jdFtBjSAE6",
    "outputId": "f0f393c5-4369-422c-9aef-fc290ccc941d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1536, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = models.resnet50(pretrained=True)\n",
    "#model = timm.create_model('resnet50', pretrained=True)\n",
    "model = timm.create_model('mixnet_xl', pretrained=True)\n",
    "#model.fc #show fully connected layer for ResNet family\n",
    "model.classifier #show the classifier layer (fully connected layer) for EfficientNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226454,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "w6QP4CFPBNzg",
    "outputId": "6beb0600-5fdf-4ae6-a216-40c32a13bb9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters of the model is: 14015482\n"
     ]
    }
   ],
   "source": [
    "# Create classifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "# define `classifier` for ResNet\n",
    "# Otherwise, define `fc` for EfficientNet family \n",
    "#because the definition of the full connection/classifier of 2 CNN families is differnt\n",
    "fc = nn.Sequential(OrderedDict([('fc1', nn.Linear(1536, 1000, bias=True)),\n",
    "\t\t\t\t\t\t\t     ('BN1', nn.BatchNorm2d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('dropout1', nn.Dropout(0.7)),\n",
    "                                 ('fc2', nn.Linear(1000, 512)),\n",
    "\t\t\t\t\t\t\t\t ('BN2', nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('swish1', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('dropout2', nn.Dropout(0.5)),\n",
    "\t\t\t\t\t\t\t\t ('fc3', nn.Linear(512, 128)),\n",
    "\t\t\t\t\t\t\t\t ('BN3', nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t     ('swish2', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('fc4', nn.Linear(128, 2)),\n",
    "\t\t\t\t\t\t\t\t ('output', nn.Softmax(dim=1))\n",
    "\t\t\t\t\t\t\t ]))\n",
    "# connect base model (EfficientNet_B0) with modified classifier layer\n",
    "model.fc = fc\n",
    "criterion = LabelSmoothingCrossEntropy()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = Nadam(model.parameters(), lr=0.001)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=0.01,momentum=0.9,\n",
    "                      nesterov=True,\n",
    "                      weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "#show our model architechture and send to GPU\n",
    "model.to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count = count_parameters(model)\n",
    "print(\"The number of parameters of the model is:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPNx-TodPpVA"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=200, checkpoint = None):\n",
    "    since = time.time()\n",
    "\n",
    "    if checkpoint is None:\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_loss = math.inf\n",
    "        best_acc = 0.\n",
    "    else:\n",
    "        print(f'Val loss: {checkpoint[\"best_val_loss\"]}, Val accuracy: {checkpoint[\"best_val_accuracy\"]}')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best_loss = checkpoint['best_val_loss']\n",
    "        best_acc = checkpoint['best_val_accuracy']\n",
    "   \n",
    "    # Tensorboard summary\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(data_loader[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if i % 1000 == 999:\n",
    "                    print('[%d, %d] loss: %.8f' % \n",
    "                          (epoch + 1, i, running_loss / (i * inputs.size(0))))\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':                \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if phase == 'train':                \n",
    "                scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.8f} Acc: {:.8f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # Record training loss and accuracy for each phase\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('Train/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Train/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "            else:\n",
    "                writer.add_scalar('Valid/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Valid/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "            # deep copy the model\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                print(f'New best model found!')\n",
    "                print(f'New record ACC: {epoch_acc}, previous record acc: {best_acc}')\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save({'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_val_loss': best_loss,\n",
    "                            'best_val_accuracy': best_acc,\n",
    "                            'scheduler_state_dict' : scheduler.state_dict(),\n",
    "                            }, \n",
    "                            CHECK_POINT_PATH\n",
    "                            )\n",
    "                print(f'New record acc is SAVED: {epoch_acc}')\n",
    "\n",
    "        print()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.8f} Best val loss: {:.8f}'.format(best_acc, best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_loss, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "vcXkJFOlP4NJ",
    "outputId": "e47fadb8-c292-4051-8a56-bbdc5868abe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.48 µs\n",
      "checkpoint loaded\n",
      "Val loss: 0.16318205505609512, Val accuracy: 0.9600000000000001\n",
      "Epoch 0/299\n",
      "----------\n",
      "train Loss: 2.34345789 Acc: 0.88575900\n",
      "val Loss: 2.56195465 Acc: 0.94666667\n",
      "\n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: 2.35545343 Acc: 0.87010955\n",
      "val Loss: 2.55541972 Acc: 0.94666667\n",
      "\n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: 2.31091136 Acc: 0.89984351\n",
      "val Loss: 2.53978498 Acc: 0.96000000\n",
      "\n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: 2.34421301 Acc: 0.88888889\n",
      "val Loss: 2.55662718 Acc: 0.94666667\n",
      "\n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: 2.33112793 Acc: 0.88732394\n",
      "val Loss: 2.53323163 Acc: 0.94666667\n",
      "\n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: 2.31735722 Acc: 0.89201878\n",
      "val Loss: 2.55026492 Acc: 0.94666667\n",
      "\n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: 2.32586348 Acc: 0.88419405\n",
      "val Loss: 2.56664710 Acc: 0.94666667\n",
      "\n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: 2.35395832 Acc: 0.88262911\n",
      "val Loss: 2.56975415 Acc: 0.94666667\n",
      "\n",
      "Epoch 8/299\n",
      "----------\n",
      "train Loss: 2.36478884 Acc: 0.89045383\n",
      "val Loss: 2.55072888 Acc: 0.94666667\n",
      "\n",
      "Epoch 9/299\n",
      "----------\n",
      "train Loss: 2.35340446 Acc: 0.89201878\n",
      "val Loss: 2.56026730 Acc: 0.96000000\n",
      "\n",
      "Epoch 10/299\n",
      "----------\n",
      "train Loss: 2.30873801 Acc: 0.90923318\n",
      "val Loss: 2.55427229 Acc: 0.94666667\n",
      "\n",
      "Epoch 11/299\n",
      "----------\n",
      "train Loss: 2.28651873 Acc: 0.92175274\n",
      "val Loss: 2.52408958 Acc: 0.94666667\n",
      "\n",
      "Epoch 12/299\n",
      "----------\n",
      "train Loss: 2.34432050 Acc: 0.88888889\n",
      "val Loss: 2.52335944 Acc: 0.94666667\n",
      "\n",
      "Epoch 13/299\n",
      "----------\n",
      "train Loss: 2.36163858 Acc: 0.87636933\n",
      "val Loss: 2.56973337 Acc: 0.94666667\n",
      "\n",
      "Epoch 14/299\n",
      "----------\n",
      "train Loss: 2.33292331 Acc: 0.87636933\n",
      "val Loss: 2.57670979 Acc: 0.94666667\n",
      "\n",
      "Epoch 15/299\n",
      "----------\n",
      "train Loss: 2.32964013 Acc: 0.89984351\n",
      "val Loss: 2.56412171 Acc: 0.94666667\n",
      "\n",
      "Epoch 16/299\n",
      "----------\n",
      "train Loss: 2.33399540 Acc: 0.89358372\n",
      "val Loss: 2.55238145 Acc: 0.96000000\n",
      "\n",
      "Epoch 17/299\n",
      "----------\n",
      "train Loss: 2.32942754 Acc: 0.89671362\n",
      "val Loss: 2.53714568 Acc: 0.94666667\n",
      "\n",
      "Epoch 18/299\n",
      "----------\n",
      "train Loss: 2.33329608 Acc: 0.89827856\n",
      "val Loss: 2.55134763 Acc: 0.94666667\n",
      "\n",
      "Epoch 19/299\n",
      "----------\n",
      "train Loss: 2.33792489 Acc: 0.88419405\n",
      "val Loss: 2.55759478 Acc: 0.93333333\n",
      "\n",
      "Epoch 20/299\n",
      "----------\n",
      "train Loss: 2.36263855 Acc: 0.87480438\n",
      "val Loss: 2.55163232 Acc: 0.93333333\n",
      "\n",
      "Epoch 21/299\n",
      "----------\n",
      "train Loss: 2.35173201 Acc: 0.89201878\n",
      "val Loss: 2.54776926 Acc: 0.96000000\n",
      "\n",
      "Epoch 22/299\n",
      "----------\n",
      "train Loss: 2.34065867 Acc: 0.89514867\n",
      "val Loss: 2.52881995 Acc: 0.94666667\n",
      "\n",
      "Epoch 23/299\n",
      "----------\n",
      "train Loss: 2.32949207 Acc: 0.89045383\n",
      "val Loss: 2.51682263 Acc: 0.94666667\n",
      "\n",
      "Epoch 24/299\n",
      "----------\n",
      "train Loss: 2.33034108 Acc: 0.89827856\n",
      "val Loss: 2.52206453 Acc: 0.94666667\n",
      "\n",
      "Epoch 25/299\n",
      "----------\n",
      "train Loss: 2.32019111 Acc: 0.89827856\n",
      "val Loss: 2.51886264 Acc: 0.96000000\n",
      "\n",
      "Epoch 26/299\n",
      "----------\n",
      "train Loss: 2.33898080 Acc: 0.89984351\n",
      "val Loss: 2.55471475 Acc: 0.94666667\n",
      "\n",
      "Epoch 27/299\n",
      "----------\n",
      "train Loss: 2.34001398 Acc: 0.89514867\n",
      "val Loss: 2.54512759 Acc: 0.94666667\n",
      "\n",
      "Epoch 28/299\n",
      "----------\n",
      "train Loss: 2.32229260 Acc: 0.89045383\n",
      "val Loss: 2.58821926 Acc: 0.94666667\n",
      "\n",
      "Epoch 29/299\n",
      "----------\n",
      "train Loss: 2.36690565 Acc: 0.86854460\n",
      "val Loss: 2.56331198 Acc: 0.94666667\n",
      "\n",
      "Epoch 30/299\n",
      "----------\n",
      "train Loss: 2.35756054 Acc: 0.87636933\n",
      "val Loss: 2.55124214 Acc: 0.93333333\n",
      "\n",
      "Epoch 31/299\n",
      "----------\n",
      "train Loss: 2.34185454 Acc: 0.88419405\n",
      "val Loss: 2.52321170 Acc: 0.94666667\n",
      "\n",
      "Epoch 32/299\n",
      "----------\n",
      "train Loss: 2.32351849 Acc: 0.89671362\n",
      "val Loss: 2.50719921 Acc: 0.94666667\n",
      "\n",
      "Epoch 33/299\n",
      "----------\n",
      "train Loss: 2.32340801 Acc: 0.90297340\n",
      "val Loss: 2.52833211 Acc: 0.94666667\n",
      "\n",
      "Epoch 34/299\n",
      "----------\n",
      "train Loss: 2.32219895 Acc: 0.88575900\n",
      "val Loss: 2.56538076 Acc: 0.94666667\n",
      "\n",
      "Epoch 35/299\n",
      "----------\n",
      "train Loss: 2.33802923 Acc: 0.88888889\n",
      "val Loss: 2.52746033 Acc: 0.96000000\n",
      "\n",
      "Epoch 36/299\n",
      "----------\n",
      "train Loss: 2.31446110 Acc: 0.89827856\n",
      "val Loss: 2.54281548 Acc: 0.93333333\n",
      "\n",
      "Epoch 37/299\n",
      "----------\n",
      "train Loss: 2.31365945 Acc: 0.90297340\n",
      "val Loss: 2.50824655 Acc: 0.96000000\n",
      "\n",
      "Epoch 38/299\n",
      "----------\n",
      "train Loss: 2.31992258 Acc: 0.90140845\n",
      "val Loss: 2.55072623 Acc: 0.93333333\n",
      "\n",
      "Epoch 39/299\n",
      "----------\n",
      "train Loss: 2.32931525 Acc: 0.88106416\n",
      "val Loss: 2.53526284 Acc: 0.93333333\n",
      "\n",
      "Epoch 40/299\n",
      "----------\n",
      "train Loss: 2.35769913 Acc: 0.87323944\n",
      "val Loss: 2.57082601 Acc: 0.94666667\n",
      "\n",
      "Epoch 41/299\n",
      "----------\n",
      "train Loss: 2.36072331 Acc: 0.88262911\n",
      "val Loss: 2.55419685 Acc: 0.94666667\n",
      "\n",
      "Epoch 42/299\n",
      "----------\n",
      "train Loss: 2.30881252 Acc: 0.90297340\n",
      "val Loss: 2.54559524 Acc: 0.94666667\n",
      "\n",
      "Epoch 43/299\n",
      "----------\n",
      "train Loss: 2.32535565 Acc: 0.90140845\n",
      "val Loss: 2.53904426 Acc: 0.94666667\n",
      "\n",
      "Epoch 44/299\n",
      "----------\n",
      "train Loss: 2.33509463 Acc: 0.90766823\n",
      "val Loss: 2.55277237 Acc: 0.96000000\n",
      "\n",
      "Epoch 45/299\n",
      "----------\n",
      "train Loss: 2.31984310 Acc: 0.90297340\n",
      "val Loss: 2.54225659 Acc: 0.96000000\n",
      "\n",
      "Epoch 46/299\n",
      "----------\n",
      "train Loss: 2.32733001 Acc: 0.89671362\n",
      "val Loss: 2.53928457 Acc: 0.94666667\n",
      "\n",
      "Epoch 47/299\n",
      "----------\n",
      "train Loss: 2.31678675 Acc: 0.89514867\n",
      "val Loss: 2.52071552 Acc: 0.94666667\n",
      "\n",
      "Epoch 48/299\n",
      "----------\n",
      "train Loss: 2.33476402 Acc: 0.87793427\n",
      "val Loss: 2.52945206 Acc: 0.94666667\n",
      "\n",
      "Epoch 49/299\n",
      "----------\n",
      "train Loss: 2.31184823 Acc: 0.89671362\n",
      "val Loss: 2.55392513 Acc: 0.96000000\n",
      "\n",
      "Epoch 50/299\n",
      "----------\n",
      "train Loss: 2.32058672 Acc: 0.90766823\n",
      "val Loss: 2.52852867 Acc: 0.94666667\n",
      "\n",
      "Epoch 51/299\n",
      "----------\n",
      "train Loss: 2.31757837 Acc: 0.88262911\n",
      "val Loss: 2.55838024 Acc: 0.96000000\n",
      "\n",
      "Epoch 52/299\n",
      "----------\n",
      "train Loss: 2.33715095 Acc: 0.89201878\n",
      "val Loss: 2.52596276 Acc: 0.96000000\n",
      "\n",
      "Epoch 53/299\n",
      "----------\n",
      "train Loss: 2.36387131 Acc: 0.87949922\n",
      "val Loss: 2.56323325 Acc: 0.94666667\n",
      "\n",
      "Epoch 54/299\n",
      "----------\n",
      "train Loss: 2.36478835 Acc: 0.87323944\n",
      "val Loss: 2.56287118 Acc: 0.94666667\n",
      "\n",
      "Epoch 55/299\n",
      "----------\n",
      "train Loss: 2.29794253 Acc: 0.90766823\n",
      "val Loss: 2.53321402 Acc: 0.96000000\n",
      "\n",
      "Epoch 56/299\n",
      "----------\n",
      "train Loss: 2.32759803 Acc: 0.90453834\n",
      "val Loss: 2.50883102 Acc: 0.96000000\n",
      "\n",
      "Epoch 57/299\n",
      "----------\n",
      "train Loss: 2.30469447 Acc: 0.91236307\n",
      "val Loss: 2.52686604 Acc: 0.94666667\n",
      "\n",
      "Epoch 58/299\n",
      "----------\n",
      "train Loss: 2.33244711 Acc: 0.88575900\n",
      "val Loss: 2.54037475 Acc: 0.94666667\n",
      "\n",
      "Epoch 59/299\n",
      "----------\n",
      "train Loss: 2.31329492 Acc: 0.89358372\n",
      "val Loss: 2.56893727 Acc: 0.94666667\n",
      "\n",
      "Epoch 60/299\n",
      "----------\n",
      "train Loss: 2.35012382 Acc: 0.88262911\n",
      "val Loss: 2.53075176 Acc: 0.96000000\n",
      "\n",
      "Epoch 61/299\n",
      "----------\n",
      "train Loss: 2.36050807 Acc: 0.87010955\n",
      "val Loss: 2.54555572 Acc: 0.94666667\n",
      "\n",
      "Epoch 62/299\n",
      "----------\n",
      "train Loss: 2.31914936 Acc: 0.89045383\n",
      "val Loss: 2.51702074 Acc: 0.94666667\n",
      "\n",
      "Epoch 63/299\n",
      "----------\n",
      "train Loss: 2.36114164 Acc: 0.88262911\n",
      "val Loss: 2.54231039 Acc: 0.94666667\n",
      "\n",
      "Epoch 64/299\n",
      "----------\n",
      "train Loss: 2.33828987 Acc: 0.88106416\n",
      "val Loss: 2.56509972 Acc: 0.94666667\n",
      "\n",
      "Epoch 65/299\n",
      "----------\n",
      "train Loss: 2.34020929 Acc: 0.88732394\n",
      "val Loss: 2.53473879 Acc: 0.94666667\n",
      "\n",
      "Epoch 66/299\n",
      "----------\n",
      "train Loss: 2.35396137 Acc: 0.87480438\n",
      "val Loss: 2.54482575 Acc: 0.94666667\n",
      "\n",
      "Epoch 67/299\n",
      "----------\n",
      "train Loss: 2.33228162 Acc: 0.90297340\n",
      "val Loss: 2.54474491 Acc: 0.96000000\n",
      "\n",
      "Epoch 68/299\n",
      "----------\n",
      "train Loss: 2.31481811 Acc: 0.90766823\n",
      "val Loss: 2.53151648 Acc: 0.94666667\n",
      "\n",
      "Epoch 69/299\n",
      "----------\n",
      "train Loss: 2.34772623 Acc: 0.88419405\n",
      "val Loss: 2.54717707 Acc: 0.94666667\n",
      "\n",
      "Epoch 70/299\n",
      "----------\n",
      "train Loss: 2.34582866 Acc: 0.87949922\n",
      "val Loss: 2.54508677 Acc: 0.93333333\n",
      "\n",
      "Epoch 71/299\n",
      "----------\n",
      "train Loss: 2.31241334 Acc: 0.88732394\n",
      "val Loss: 2.54760796 Acc: 0.94666667\n",
      "\n",
      "Epoch 72/299\n",
      "----------\n",
      "train Loss: 2.31326074 Acc: 0.89045383\n",
      "val Loss: 2.56441024 Acc: 0.94666667\n",
      "\n",
      "Epoch 73/299\n",
      "----------\n",
      "train Loss: 2.35070788 Acc: 0.88106416\n",
      "val Loss: 2.55505967 Acc: 0.94666667\n",
      "\n",
      "Epoch 74/299\n",
      "----------\n",
      "train Loss: 2.34865524 Acc: 0.89984351\n",
      "val Loss: 2.52212972 Acc: 0.93333333\n",
      "\n",
      "Epoch 75/299\n",
      "----------\n",
      "train Loss: 2.35080078 Acc: 0.89045383\n",
      "val Loss: 2.53671020 Acc: 0.94666667\n",
      "\n",
      "Epoch 76/299\n",
      "----------\n",
      "train Loss: 2.31020460 Acc: 0.89514867\n",
      "val Loss: 2.52143387 Acc: 0.96000000\n",
      "\n",
      "Epoch 77/299\n",
      "----------\n",
      "train Loss: 2.35815678 Acc: 0.87167449\n",
      "val Loss: 2.54721203 Acc: 0.94666667\n",
      "\n",
      "Epoch 78/299\n",
      "----------\n",
      "train Loss: 2.30866228 Acc: 0.89671362\n",
      "val Loss: 2.54310739 Acc: 0.96000000\n",
      "\n",
      "Epoch 79/299\n",
      "----------\n",
      "train Loss: 2.34908059 Acc: 0.87480438\n",
      "val Loss: 2.56054161 Acc: 0.94666667\n",
      "\n",
      "Epoch 80/299\n",
      "----------\n",
      "train Loss: 2.31880677 Acc: 0.90766823\n",
      "val Loss: 2.53163519 Acc: 0.93333333\n",
      "\n",
      "Epoch 81/299\n",
      "----------\n",
      "train Loss: 2.31964948 Acc: 0.89201878\n",
      "val Loss: 2.57021374 Acc: 0.93333333\n",
      "\n",
      "Epoch 82/299\n",
      "----------\n",
      "train Loss: 2.33425284 Acc: 0.88575900\n",
      "val Loss: 2.54220882 Acc: 0.96000000\n",
      "\n",
      "Epoch 83/299\n",
      "----------\n",
      "train Loss: 2.34227360 Acc: 0.88732394\n",
      "val Loss: 2.54031331 Acc: 0.94666667\n",
      "\n",
      "Epoch 84/299\n",
      "----------\n",
      "train Loss: 2.33987583 Acc: 0.89514867\n",
      "val Loss: 2.53301617 Acc: 0.94666667\n",
      "\n",
      "Epoch 85/299\n",
      "----------\n",
      "train Loss: 2.34059457 Acc: 0.88419405\n",
      "val Loss: 2.55202062 Acc: 0.94666667\n",
      "\n",
      "Epoch 86/299\n",
      "----------\n",
      "train Loss: 2.35562384 Acc: 0.87949922\n",
      "val Loss: 2.53677372 Acc: 0.96000000\n",
      "\n",
      "Epoch 87/299\n",
      "----------\n",
      "train Loss: 2.34680331 Acc: 0.87480438\n",
      "val Loss: 2.51224488 Acc: 0.96000000\n",
      "\n",
      "Epoch 88/299\n",
      "----------\n",
      "train Loss: 2.31711845 Acc: 0.90610329\n",
      "val Loss: 2.52820939 Acc: 0.96000000\n",
      "\n",
      "Epoch 89/299\n",
      "----------\n",
      "train Loss: 2.33202053 Acc: 0.88106416\n",
      "val Loss: 2.54005781 Acc: 0.94666667\n",
      "\n",
      "Epoch 90/299\n",
      "----------\n",
      "train Loss: 2.34966137 Acc: 0.86541471\n",
      "val Loss: 2.52489387 Acc: 0.93333333\n",
      "\n",
      "Epoch 91/299\n",
      "----------\n",
      "train Loss: 2.32412920 Acc: 0.91236307\n",
      "val Loss: 2.53935528 Acc: 0.93333333\n",
      "\n",
      "Epoch 92/299\n",
      "----------\n",
      "train Loss: 2.32199284 Acc: 0.90140845\n",
      "val Loss: 2.56126420 Acc: 0.94666667\n",
      "\n",
      "Epoch 93/299\n",
      "----------\n",
      "train Loss: 2.30358048 Acc: 0.91236307\n",
      "val Loss: 2.55216458 Acc: 0.94666667\n",
      "\n",
      "Epoch 94/299\n",
      "----------\n",
      "train Loss: 2.34726114 Acc: 0.88888889\n",
      "val Loss: 2.54396695 Acc: 0.94666667\n",
      "\n",
      "Epoch 95/299\n",
      "----------\n",
      "train Loss: 2.34211178 Acc: 0.88575900\n",
      "val Loss: 2.54754802 Acc: 0.94666667\n",
      "\n",
      "Epoch 96/299\n",
      "----------\n",
      "train Loss: 2.35028840 Acc: 0.88419405\n",
      "val Loss: 2.55852644 Acc: 0.94666667\n",
      "\n",
      "Epoch 97/299\n",
      "----------\n",
      "train Loss: 2.31761193 Acc: 0.89671362\n",
      "val Loss: 2.53998905 Acc: 0.94666667\n",
      "\n",
      "Epoch 98/299\n",
      "----------\n",
      "train Loss: 2.32010722 Acc: 0.89827856\n",
      "val Loss: 2.53045787 Acc: 0.94666667\n",
      "\n",
      "Epoch 99/299\n",
      "----------\n",
      "train Loss: 2.30574156 Acc: 0.89984351\n",
      "val Loss: 2.50870900 Acc: 0.94666667\n",
      "\n",
      "Epoch 100/299\n",
      "----------\n",
      "train Loss: 2.31508652 Acc: 0.90140845\n",
      "val Loss: 2.51233078 Acc: 0.94666667\n",
      "\n",
      "Epoch 101/299\n",
      "----------\n",
      "train Loss: 2.32534979 Acc: 0.90140845\n",
      "val Loss: 2.52395466 Acc: 0.96000000\n",
      "\n",
      "Epoch 102/299\n",
      "----------\n",
      "train Loss: 2.36124588 Acc: 0.86854460\n",
      "val Loss: 2.57386362 Acc: 0.94666667\n",
      "\n",
      "Epoch 103/299\n",
      "----------\n",
      "train Loss: 2.34728359 Acc: 0.88888889\n",
      "val Loss: 2.52903491 Acc: 0.94666667\n",
      "\n",
      "Epoch 104/299\n",
      "----------\n",
      "train Loss: 2.32802506 Acc: 0.90923318\n",
      "val Loss: 2.53009022 Acc: 0.94666667\n",
      "\n",
      "Epoch 105/299\n",
      "----------\n",
      "train Loss: 2.33896072 Acc: 0.87167449\n",
      "val Loss: 2.51314810 Acc: 0.94666667\n",
      "\n",
      "Epoch 106/299\n",
      "----------\n",
      "train Loss: 2.35929603 Acc: 0.87793427\n",
      "val Loss: 2.54756035 Acc: 0.94666667\n",
      "\n",
      "Epoch 107/299\n",
      "----------\n",
      "train Loss: 2.32860851 Acc: 0.89514867\n",
      "val Loss: 2.53741841 Acc: 0.93333333\n",
      "\n",
      "Epoch 108/299\n",
      "----------\n",
      "train Loss: 2.34143236 Acc: 0.89045383\n",
      "val Loss: 2.55378655 Acc: 0.93333333\n",
      "\n",
      "Epoch 109/299\n",
      "----------\n",
      "train Loss: 2.32225881 Acc: 0.90923318\n",
      "val Loss: 2.58627743 Acc: 0.94666667\n",
      "\n",
      "Epoch 110/299\n",
      "----------\n",
      "train Loss: 2.35185300 Acc: 0.88575900\n",
      "val Loss: 2.53433140 Acc: 0.94666667\n",
      "\n",
      "Epoch 111/299\n",
      "----------\n",
      "train Loss: 2.32512937 Acc: 0.89671362\n",
      "val Loss: 2.55318366 Acc: 0.94666667\n",
      "\n",
      "Epoch 112/299\n",
      "----------\n",
      "train Loss: 2.34376263 Acc: 0.87636933\n",
      "val Loss: 2.54395124 Acc: 0.96000000\n",
      "\n",
      "Epoch 113/299\n",
      "----------\n",
      "train Loss: 2.32342447 Acc: 0.89045383\n",
      "val Loss: 2.54143107 Acc: 0.94666667\n",
      "\n",
      "Epoch 114/299\n",
      "----------\n",
      "train Loss: 2.32701134 Acc: 0.89201878\n",
      "val Loss: 2.55290725 Acc: 0.94666667\n",
      "\n",
      "Epoch 115/299\n",
      "----------\n",
      "train Loss: 2.36086815 Acc: 0.87323944\n",
      "val Loss: 2.55357386 Acc: 0.94666667\n",
      "\n",
      "Epoch 116/299\n",
      "----------\n",
      "train Loss: 2.32692891 Acc: 0.89358372\n",
      "val Loss: 2.59648080 Acc: 0.94666667\n",
      "\n",
      "Epoch 117/299\n",
      "----------\n",
      "train Loss: 2.33738430 Acc: 0.88575900\n",
      "val Loss: 2.57407305 Acc: 0.94666667\n",
      "\n",
      "Epoch 118/299\n",
      "----------\n",
      "train Loss: 2.33624740 Acc: 0.89827856\n",
      "val Loss: 2.53017916 Acc: 0.94666667\n",
      "\n",
      "Epoch 119/299\n",
      "----------\n",
      "train Loss: 2.33926488 Acc: 0.87793427\n",
      "val Loss: 2.55929113 Acc: 0.94666667\n",
      "\n",
      "Epoch 120/299\n",
      "----------\n",
      "train Loss: 2.34594858 Acc: 0.87010955\n",
      "val Loss: 2.55477693 Acc: 0.94666667\n",
      "\n",
      "Epoch 121/299\n",
      "----------\n",
      "train Loss: 2.32661127 Acc: 0.89671362\n",
      "val Loss: 2.53781184 Acc: 0.96000000\n",
      "\n",
      "Epoch 122/299\n",
      "----------\n",
      "train Loss: 2.33078144 Acc: 0.88419405\n",
      "val Loss: 2.55462755 Acc: 0.94666667\n",
      "\n",
      "Epoch 123/299\n",
      "----------\n",
      "train Loss: 2.37009391 Acc: 0.86697966\n",
      "val Loss: 2.55338684 Acc: 0.96000000\n",
      "\n",
      "Epoch 124/299\n",
      "----------\n",
      "train Loss: 2.32582558 Acc: 0.88888889\n",
      "val Loss: 2.51057854 Acc: 0.94666667\n",
      "\n",
      "Epoch 125/299\n",
      "----------\n",
      "train Loss: 2.33496060 Acc: 0.89827856\n",
      "val Loss: 2.50423320 Acc: 0.94666667\n",
      "\n",
      "Epoch 126/299\n",
      "----------\n",
      "train Loss: 2.33767768 Acc: 0.89984351\n",
      "val Loss: 2.51919291 Acc: 0.94666667\n",
      "\n",
      "Epoch 127/299\n",
      "----------\n",
      "train Loss: 2.35161995 Acc: 0.87010955\n",
      "val Loss: 2.52095705 Acc: 0.94666667\n",
      "\n",
      "Epoch 128/299\n",
      "----------\n",
      "train Loss: 2.31717534 Acc: 0.89984351\n",
      "val Loss: 2.54017332 Acc: 0.94666667\n",
      "\n",
      "Epoch 129/299\n",
      "----------\n",
      "train Loss: 2.33948494 Acc: 0.88732394\n",
      "val Loss: 2.53373359 Acc: 0.94666667\n",
      "\n",
      "Epoch 130/299\n",
      "----------\n",
      "train Loss: 2.31973608 Acc: 0.89201878\n",
      "val Loss: 2.53822771 Acc: 0.94666667\n",
      "\n",
      "Epoch 131/299\n",
      "----------\n",
      "train Loss: 2.33975854 Acc: 0.88888889\n",
      "val Loss: 2.55359534 Acc: 0.94666667\n",
      "\n",
      "Epoch 132/299\n",
      "----------\n",
      "train Loss: 2.33651235 Acc: 0.89984351\n",
      "val Loss: 2.54450264 Acc: 0.94666667\n",
      "\n",
      "Epoch 133/299\n",
      "----------\n",
      "train Loss: 2.33873669 Acc: 0.88106416\n",
      "val Loss: 2.55439602 Acc: 0.94666667\n",
      "\n",
      "Epoch 134/299\n",
      "----------\n",
      "train Loss: 2.31713302 Acc: 0.90453834\n",
      "val Loss: 2.54733746 Acc: 0.94666667\n",
      "\n",
      "Epoch 135/299\n",
      "----------\n",
      "train Loss: 2.31726952 Acc: 0.90140845\n",
      "val Loss: 2.55068861 Acc: 0.94666667\n",
      "\n",
      "Epoch 136/299\n",
      "----------\n",
      "train Loss: 2.32106908 Acc: 0.89514867\n",
      "val Loss: 2.53227876 Acc: 0.94666667\n",
      "\n",
      "Epoch 137/299\n",
      "----------\n",
      "train Loss: 2.33539050 Acc: 0.88106416\n",
      "val Loss: 2.53094865 Acc: 0.94666667\n",
      "\n",
      "Epoch 138/299\n",
      "----------\n",
      "train Loss: 2.32415461 Acc: 0.91236307\n",
      "val Loss: 2.52302814 Acc: 0.94666667\n",
      "\n",
      "Epoch 139/299\n",
      "----------\n",
      "train Loss: 2.31023206 Acc: 0.90610329\n",
      "val Loss: 2.52917205 Acc: 0.94666667\n",
      "\n",
      "Epoch 140/299\n",
      "----------\n",
      "train Loss: 2.34874233 Acc: 0.87793427\n",
      "val Loss: 2.54002970 Acc: 0.94666667\n",
      "\n",
      "Epoch 141/299\n",
      "----------\n",
      "train Loss: 2.35031649 Acc: 0.88888889\n",
      "val Loss: 2.55438097 Acc: 0.96000000\n",
      "\n",
      "Epoch 142/299\n",
      "----------\n",
      "train Loss: 2.34196530 Acc: 0.88888889\n",
      "val Loss: 2.54386836 Acc: 0.93333333\n",
      "\n",
      "Epoch 143/299\n",
      "----------\n",
      "train Loss: 2.30944394 Acc: 0.90140845\n",
      "val Loss: 2.55341906 Acc: 0.96000000\n",
      "\n",
      "Epoch 144/299\n",
      "----------\n",
      "train Loss: 2.31253897 Acc: 0.88888889\n",
      "val Loss: 2.54859236 Acc: 0.94666667\n",
      "\n",
      "Epoch 145/299\n",
      "----------\n",
      "train Loss: 2.31141394 Acc: 0.91862285\n",
      "val Loss: 2.53187638 Acc: 0.96000000\n",
      "\n",
      "Epoch 146/299\n",
      "----------\n",
      "train Loss: 2.35612082 Acc: 0.88732394\n",
      "val Loss: 2.54900438 Acc: 0.94666667\n",
      "\n",
      "Epoch 147/299\n",
      "----------\n",
      "train Loss: 2.33408506 Acc: 0.88732394\n",
      "val Loss: 2.56289182 Acc: 0.94666667\n",
      "\n",
      "Epoch 148/299\n",
      "----------\n",
      "train Loss: 2.34484771 Acc: 0.89201878\n",
      "val Loss: 2.57518037 Acc: 0.94666667\n",
      "\n",
      "Epoch 149/299\n",
      "----------\n",
      "train Loss: 2.35137206 Acc: 0.88575900\n",
      "val Loss: 2.52578400 Acc: 0.93333333\n",
      "\n",
      "Epoch 150/299\n",
      "----------\n",
      "train Loss: 2.33761326 Acc: 0.89201878\n",
      "val Loss: 2.54662936 Acc: 0.96000000\n",
      "\n",
      "Epoch 151/299\n",
      "----------\n",
      "train Loss: 2.32823852 Acc: 0.87167449\n",
      "val Loss: 2.51477331 Acc: 0.93333333\n",
      "\n",
      "Epoch 152/299\n",
      "----------\n",
      "train Loss: 2.34544895 Acc: 0.89984351\n",
      "val Loss: 2.55788262 Acc: 0.94666667\n",
      "\n",
      "Epoch 153/299\n",
      "----------\n",
      "train Loss: 2.34704797 Acc: 0.88106416\n",
      "val Loss: 2.57618721 Acc: 0.94666667\n",
      "\n",
      "Epoch 154/299\n",
      "----------\n",
      "train Loss: 2.33612760 Acc: 0.89201878\n",
      "val Loss: 2.54536720 Acc: 0.94666667\n",
      "\n",
      "Epoch 155/299\n",
      "----------\n",
      "train Loss: 2.32365283 Acc: 0.89827856\n",
      "val Loss: 2.53589730 Acc: 0.94666667\n",
      "\n",
      "Epoch 156/299\n",
      "----------\n",
      "train Loss: 2.32086191 Acc: 0.90766823\n",
      "val Loss: 2.53572954 Acc: 0.94666667\n",
      "\n",
      "Epoch 157/299\n",
      "----------\n",
      "train Loss: 2.31084288 Acc: 0.89827856\n",
      "val Loss: 2.51816022 Acc: 0.96000000\n",
      "\n",
      "Epoch 158/299\n",
      "----------\n",
      "train Loss: 2.31479887 Acc: 0.90766823\n",
      "val Loss: 2.49055123 Acc: 0.94666667\n",
      "\n",
      "Epoch 159/299\n",
      "----------\n",
      "train Loss: 2.34857363 Acc: 0.87167449\n",
      "val Loss: 2.53612632 Acc: 0.96000000\n",
      "\n",
      "Epoch 160/299\n",
      "----------\n",
      "train Loss: 2.31982210 Acc: 0.89201878\n",
      "val Loss: 2.56064559 Acc: 0.93333333\n",
      "\n",
      "Epoch 161/299\n",
      "----------\n",
      "train Loss: 2.30161536 Acc: 0.90610329\n",
      "val Loss: 2.54946006 Acc: 0.94666667\n",
      "\n",
      "Epoch 162/299\n",
      "----------\n",
      "train Loss: 2.31396563 Acc: 0.89827856\n",
      "val Loss: 2.51158914 Acc: 0.96000000\n",
      "\n",
      "Epoch 163/299\n",
      "----------\n",
      "train Loss: 2.33804751 Acc: 0.87949922\n",
      "val Loss: 2.55481373 Acc: 0.93333333\n",
      "\n",
      "Epoch 164/299\n",
      "----------\n",
      "train Loss: 2.33064127 Acc: 0.89514867\n",
      "val Loss: 2.56284270 Acc: 0.94666667\n",
      "\n",
      "Epoch 165/299\n",
      "----------\n",
      "train Loss: 2.33006830 Acc: 0.88888889\n",
      "val Loss: 2.53989805 Acc: 0.94666667\n",
      "\n",
      "Epoch 166/299\n",
      "----------\n",
      "train Loss: 2.31297552 Acc: 0.90766823\n",
      "val Loss: 2.54837341 Acc: 0.94666667\n",
      "\n",
      "Epoch 167/299\n",
      "----------\n",
      "train Loss: 2.32044337 Acc: 0.89984351\n",
      "val Loss: 2.54998148 Acc: 0.94666667\n",
      "\n",
      "Epoch 168/299\n",
      "----------\n",
      "train Loss: 2.35715783 Acc: 0.89045383\n",
      "val Loss: 2.53547154 Acc: 0.94666667\n",
      "\n",
      "Epoch 169/299\n",
      "----------\n",
      "train Loss: 2.31901923 Acc: 0.90297340\n",
      "val Loss: 2.53470502 Acc: 0.94666667\n",
      "\n",
      "Epoch 170/299\n",
      "----------\n",
      "train Loss: 2.33068738 Acc: 0.89671362\n",
      "val Loss: 2.53593266 Acc: 0.93333333\n",
      "\n",
      "Epoch 171/299\n",
      "----------\n",
      "train Loss: 2.34976611 Acc: 0.88732394\n",
      "val Loss: 2.56553311 Acc: 0.94666667\n",
      "\n",
      "Epoch 172/299\n",
      "----------\n",
      "train Loss: 2.33873687 Acc: 0.89045383\n",
      "val Loss: 2.53465425 Acc: 0.94666667\n",
      "\n",
      "Epoch 173/299\n",
      "----------\n",
      "train Loss: 2.35264658 Acc: 0.87949922\n",
      "val Loss: 2.52239498 Acc: 0.94666667\n",
      "\n",
      "Epoch 174/299\n",
      "----------\n",
      "train Loss: 2.33717908 Acc: 0.87949922\n",
      "val Loss: 2.54982948 Acc: 0.92000000\n",
      "\n",
      "Epoch 175/299\n",
      "----------\n",
      "train Loss: 2.30833968 Acc: 0.90297340\n",
      "val Loss: 2.51603838 Acc: 0.93333333\n",
      "\n",
      "Epoch 176/299\n",
      "----------\n",
      "train Loss: 2.34026099 Acc: 0.89827856\n",
      "val Loss: 2.54464739 Acc: 0.94666667\n",
      "\n",
      "Epoch 177/299\n",
      "----------\n",
      "train Loss: 2.34582604 Acc: 0.88262911\n",
      "val Loss: 2.54861805 Acc: 0.94666667\n",
      "\n",
      "Epoch 178/299\n",
      "----------\n",
      "train Loss: 2.34823731 Acc: 0.88575900\n",
      "val Loss: 2.54154409 Acc: 0.93333333\n",
      "\n",
      "Epoch 179/299\n",
      "----------\n",
      "train Loss: 2.34830438 Acc: 0.88419405\n",
      "val Loss: 2.54452387 Acc: 0.94666667\n",
      "\n",
      "Epoch 180/299\n",
      "----------\n",
      "train Loss: 2.35416988 Acc: 0.87010955\n",
      "val Loss: 2.56140247 Acc: 0.93333333\n",
      "\n",
      "Epoch 181/299\n",
      "----------\n",
      "train Loss: 2.30482294 Acc: 0.91392801\n",
      "val Loss: 2.54536210 Acc: 0.94666667\n",
      "\n",
      "Epoch 182/299\n",
      "----------\n",
      "train Loss: 2.32809658 Acc: 0.89827856\n",
      "val Loss: 2.53712522 Acc: 0.94666667\n",
      "\n",
      "Epoch 183/299\n",
      "----------\n",
      "train Loss: 2.35582346 Acc: 0.88106416\n",
      "val Loss: 2.52171100 Acc: 0.96000000\n",
      "\n",
      "Epoch 184/299\n",
      "----------\n",
      "train Loss: 2.36189521 Acc: 0.87793427\n",
      "val Loss: 2.54266063 Acc: 0.94666667\n",
      "\n",
      "Epoch 185/299\n",
      "----------\n",
      "train Loss: 2.31952142 Acc: 0.90610329\n",
      "val Loss: 2.53449481 Acc: 0.93333333\n",
      "\n",
      "Epoch 186/299\n",
      "----------\n",
      "train Loss: 2.35738615 Acc: 0.87793427\n",
      "val Loss: 2.57417237 Acc: 0.94666667\n",
      "\n",
      "Epoch 187/299\n",
      "----------\n",
      "train Loss: 2.34193319 Acc: 0.89984351\n",
      "val Loss: 2.53618063 Acc: 0.93333333\n",
      "\n",
      "Epoch 188/299\n",
      "----------\n",
      "train Loss: 2.32560228 Acc: 0.88888889\n",
      "val Loss: 2.56344983 Acc: 0.96000000\n",
      "\n",
      "Epoch 189/299\n",
      "----------\n",
      "train Loss: 2.32447242 Acc: 0.90923318\n",
      "val Loss: 2.55919614 Acc: 0.94666667\n",
      "\n",
      "Epoch 190/299\n",
      "----------\n",
      "train Loss: 2.33325687 Acc: 0.88888889\n",
      "val Loss: 2.55780373 Acc: 0.94666667\n",
      "\n",
      "Epoch 191/299\n",
      "----------\n",
      "train Loss: 2.35096352 Acc: 0.87793427\n",
      "val Loss: 2.56875788 Acc: 0.96000000\n",
      "\n",
      "Epoch 192/299\n",
      "----------\n",
      "train Loss: 2.30024675 Acc: 0.91392801\n",
      "val Loss: 2.53913011 Acc: 0.96000000\n",
      "\n",
      "Epoch 193/299\n",
      "----------\n",
      "train Loss: 2.32859023 Acc: 0.88732394\n",
      "val Loss: 2.56618229 Acc: 0.93333333\n",
      "\n",
      "Epoch 194/299\n",
      "----------\n",
      "train Loss: 2.34390792 Acc: 0.87949922\n",
      "val Loss: 2.52336962 Acc: 0.94666667\n",
      "\n",
      "Epoch 195/299\n",
      "----------\n",
      "train Loss: 2.35845760 Acc: 0.88419405\n",
      "val Loss: 2.54832066 Acc: 0.96000000\n",
      "\n",
      "Epoch 196/299\n",
      "----------\n",
      "train Loss: 2.32000193 Acc: 0.89201878\n",
      "val Loss: 2.53702669 Acc: 0.96000000\n",
      "\n",
      "Epoch 197/299\n",
      "----------\n",
      "train Loss: 2.32692897 Acc: 0.89045383\n",
      "val Loss: 2.56358408 Acc: 0.96000000\n",
      "\n",
      "Epoch 198/299\n",
      "----------\n",
      "train Loss: 2.32631255 Acc: 0.89984351\n",
      "val Loss: 2.52951762 Acc: 0.94666667\n",
      "\n",
      "Epoch 199/299\n",
      "----------\n",
      "train Loss: 2.31800924 Acc: 0.91862285\n",
      "val Loss: 2.50536824 Acc: 0.94666667\n",
      "\n",
      "Epoch 200/299\n",
      "----------\n",
      "train Loss: 2.34500829 Acc: 0.88888889\n",
      "val Loss: 2.53529275 Acc: 0.96000000\n",
      "\n",
      "Epoch 201/299\n",
      "----------\n",
      "train Loss: 2.33840081 Acc: 0.88732394\n",
      "val Loss: 2.54109549 Acc: 0.96000000\n",
      "\n",
      "Epoch 202/299\n",
      "----------\n",
      "train Loss: 2.32588540 Acc: 0.88732394\n",
      "val Loss: 2.54493381 Acc: 0.96000000\n",
      "\n",
      "Epoch 203/299\n",
      "----------\n",
      "train Loss: 2.32206955 Acc: 0.89671362\n",
      "val Loss: 2.53962601 Acc: 0.94666667\n",
      "\n",
      "Epoch 204/299\n",
      "----------\n",
      "train Loss: 2.31621621 Acc: 0.90297340\n",
      "val Loss: 2.54026407 Acc: 0.93333333\n",
      "\n",
      "Epoch 205/299\n",
      "----------\n",
      "train Loss: 2.29106935 Acc: 0.92018779\n",
      "val Loss: 2.54569163 Acc: 0.94666667\n",
      "\n",
      "Epoch 206/299\n",
      "----------\n",
      "train Loss: 2.33572806 Acc: 0.88575900\n",
      "val Loss: 2.55716972 Acc: 0.96000000\n",
      "\n",
      "Epoch 207/299\n",
      "----------\n",
      "train Loss: 2.33286335 Acc: 0.89984351\n",
      "val Loss: 2.54495044 Acc: 0.94666667\n",
      "\n",
      "Epoch 208/299\n",
      "----------\n",
      "train Loss: 2.32507322 Acc: 0.90297340\n",
      "val Loss: 2.53209285 Acc: 0.93333333\n",
      "\n",
      "Epoch 209/299\n",
      "----------\n",
      "train Loss: 2.34403503 Acc: 0.87480438\n",
      "val Loss: 2.53856841 Acc: 0.96000000\n",
      "\n",
      "Epoch 210/299\n",
      "----------\n",
      "train Loss: 2.32853057 Acc: 0.89045383\n",
      "val Loss: 2.54282431 Acc: 0.94666667\n",
      "\n",
      "Epoch 211/299\n",
      "----------\n",
      "train Loss: 2.32757860 Acc: 0.89984351\n",
      "val Loss: 2.53143207 Acc: 0.96000000\n",
      "\n",
      "Epoch 212/299\n",
      "----------\n",
      "train Loss: 2.33707050 Acc: 0.89514867\n",
      "val Loss: 2.52475577 Acc: 0.96000000\n",
      "\n",
      "Epoch 213/299\n",
      "----------\n",
      "train Loss: 2.35957951 Acc: 0.87636933\n",
      "val Loss: 2.55823049 Acc: 0.94666667\n",
      "\n",
      "Epoch 214/299\n",
      "----------\n",
      "train Loss: 2.35076336 Acc: 0.86697966\n",
      "val Loss: 2.55251354 Acc: 0.94666667\n",
      "\n",
      "Epoch 215/299\n",
      "----------\n",
      "train Loss: 2.32324851 Acc: 0.90453834\n",
      "val Loss: 2.53216279 Acc: 0.94666667\n",
      "\n",
      "Epoch 216/299\n",
      "----------\n",
      "train Loss: 2.31925739 Acc: 0.89045383\n",
      "val Loss: 2.56094486 Acc: 0.94666667\n",
      "\n",
      "Epoch 217/299\n",
      "----------\n",
      "train Loss: 2.32796584 Acc: 0.89201878\n",
      "val Loss: 2.55992826 Acc: 0.93333333\n",
      "\n",
      "Epoch 218/299\n",
      "----------\n",
      "train Loss: 2.32546393 Acc: 0.88106416\n",
      "val Loss: 2.55156173 Acc: 0.94666667\n",
      "\n",
      "Epoch 219/299\n",
      "----------\n",
      "train Loss: 2.33256995 Acc: 0.90140845\n",
      "val Loss: 2.54177250 Acc: 0.96000000\n",
      "\n",
      "Epoch 220/299\n",
      "----------\n",
      "train Loss: 2.34470287 Acc: 0.87480438\n",
      "val Loss: 2.51539818 Acc: 0.96000000\n",
      "\n",
      "Epoch 221/299\n",
      "----------\n",
      "train Loss: 2.33494832 Acc: 0.89671362\n",
      "val Loss: 2.52280631 Acc: 0.96000000\n",
      "\n",
      "Epoch 222/299\n",
      "----------\n",
      "train Loss: 2.30437163 Acc: 0.91236307\n",
      "val Loss: 2.56424719 Acc: 0.94666667\n",
      "\n",
      "Epoch 223/299\n",
      "----------\n",
      "train Loss: 2.35816212 Acc: 0.88106416\n",
      "val Loss: 2.54007368 Acc: 0.96000000\n",
      "\n",
      "Epoch 224/299\n",
      "----------\n",
      "train Loss: 2.31687201 Acc: 0.89827856\n",
      "val Loss: 2.54402472 Acc: 0.94666667\n",
      "\n",
      "Epoch 225/299\n",
      "----------\n",
      "train Loss: 2.32594776 Acc: 0.87793427\n",
      "val Loss: 2.52026510 Acc: 0.94666667\n",
      "\n",
      "Epoch 226/299\n",
      "----------\n",
      "train Loss: 2.33571199 Acc: 0.89045383\n",
      "val Loss: 2.53670926 Acc: 0.94666667\n",
      "\n",
      "Epoch 227/299\n",
      "----------\n",
      "train Loss: 2.34109019 Acc: 0.88732394\n",
      "val Loss: 2.53846605 Acc: 0.94666667\n",
      "\n",
      "Epoch 228/299\n",
      "----------\n",
      "train Loss: 2.34869253 Acc: 0.88262911\n",
      "val Loss: 2.53170604 Acc: 0.93333333\n",
      "\n",
      "Epoch 229/299\n",
      "----------\n",
      "train Loss: 2.34164738 Acc: 0.87793427\n",
      "val Loss: 2.52059090 Acc: 0.94666667\n",
      "\n",
      "Epoch 230/299\n",
      "----------\n",
      "train Loss: 2.36327581 Acc: 0.87793427\n",
      "val Loss: 2.56991667 Acc: 0.94666667\n",
      "\n",
      "Epoch 231/299\n",
      "----------\n",
      "train Loss: 2.32150520 Acc: 0.89201878\n",
      "val Loss: 2.57181126 Acc: 0.94666667\n",
      "\n",
      "Epoch 232/299\n",
      "----------\n",
      "train Loss: 2.32124481 Acc: 0.88732394\n",
      "val Loss: 2.56185485 Acc: 0.94666667\n",
      "\n",
      "Epoch 233/299\n",
      "----------\n",
      "train Loss: 2.32203426 Acc: 0.90297340\n",
      "val Loss: 2.53231608 Acc: 0.94666667\n",
      "\n",
      "Epoch 234/299\n",
      "----------\n",
      "train Loss: 2.33579465 Acc: 0.89514867\n",
      "val Loss: 2.51954305 Acc: 0.94666667\n",
      "\n",
      "Epoch 235/299\n",
      "----------\n",
      "train Loss: 2.34052215 Acc: 0.88888889\n",
      "val Loss: 2.55381749 Acc: 0.94666667\n",
      "\n",
      "Epoch 236/299\n",
      "----------\n",
      "train Loss: 2.32168939 Acc: 0.89045383\n",
      "val Loss: 2.53342819 Acc: 0.94666667\n",
      "\n",
      "Epoch 237/299\n",
      "----------\n",
      "train Loss: 2.38562323 Acc: 0.84507042\n",
      "val Loss: 2.58071710 Acc: 0.94666667\n",
      "\n",
      "Epoch 238/299\n",
      "----------\n",
      "train Loss: 2.32639094 Acc: 0.89358372\n",
      "val Loss: 2.53713887 Acc: 0.96000000\n",
      "\n",
      "Epoch 239/299\n",
      "----------\n",
      "train Loss: 2.32188616 Acc: 0.89984351\n",
      "val Loss: 2.54332032 Acc: 0.96000000\n",
      "\n",
      "Epoch 240/299\n",
      "----------\n",
      "train Loss: 2.31581770 Acc: 0.90453834\n",
      "val Loss: 2.51213620 Acc: 0.94666667\n",
      "\n",
      "Epoch 241/299\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "CHECK_POINT_PATH = '/home/linh/Downloads/Covid-19/weights_CT/MixNet_Extra_Large_Dataset_Yang.pth'\n",
    "#CHECK_POINT_PATH = '/home/linh/Downloads/Covid-19/weights_CT/MixNet_Extra_Large_Dataset_Soares.pth'\n",
    "#CHECK_POINT_PATH = '/home/linh/Downloads/Covid-19/weights_CT/MixNet_Extra_Large_Dataset_Moh.pth'\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "    print(\"checkpoint loaded\")\n",
    "except:\n",
    "    checkpoint = None\n",
    "    print(\"checkpoint not found\")\n",
    "if checkpoint == None:\n",
    "    CHECK_POINT_PATH = CHECK_POINT_PATH\n",
    "model, best_val_loss, best_val_acc = train_model(model,\n",
    "                                                 criterion,\n",
    "                                                 optimizer,\n",
    "                                                 scheduler,\n",
    "                                                 num_epochs = 300,\n",
    "                                                 checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "                                                 ) \n",
    "                                                \n",
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_val_accuracy': best_val_acc,\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            }, CHECK_POINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Covid-19_EfficientNet_B0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
