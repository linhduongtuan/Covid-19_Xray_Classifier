{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40178,
     "status": "ok",
     "timestamp": 1588213047201,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "rPwL9bdoBNzQ",
    "outputId": "553f83f0-cbf1-48d5-a184-4f4c8ff055ac"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import PIL\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import timm\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import logging\n",
    "import fnmatch\n",
    "import argparse\n",
    "import torchvision\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from timm.models.layers.activations import *\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from collections import OrderedDict, defaultdict\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from randaugment import RandAugment, ImageNetPolicy, Cutout\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 179460,
     "status": "ok",
     "timestamp": 1588213186502,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "yyGpxuktB96O",
    "outputId": "584ea32f-dbe1-4465-8e60-e0f4e5c96a6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID-19', 'NonCOVID-19']\n",
      "{'train': 639, 'val': 75}\n",
      "cuda:0\n",
      "{0: 'COVID-19', 1: 'NonCOVID-19'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([56, 3, 224, 224])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/home/linh/Downloads/Covid-19/CT/Yang'\n",
    "#data_dir = '/home/linh/Downloads/Covid-19/CT/Soares'\n",
    "#data_dir = '/home/linh/Downloads/Covid-19/CT/Moh'\n",
    "\n",
    "# Define your transforms for the training and testing sets\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        RandAugment(),\n",
    "        ImageNetPolicy(),\n",
    "        Cutout(size=16),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Load the datasets with ImageFolder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "batch_size = 56\n",
    "num_epochs = 300\n",
    "data_loader = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=8, pin_memory = True)\n",
    "              for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "print(class_names)\n",
    "print(dataset_sizes)\n",
    "print(device)\n",
    "\n",
    "### we get the class_to_index in the data_Set but what we really need is the cat_to_names  so we will create\n",
    "_ = image_datasets['train'].class_to_idx\n",
    "cat_to_name = {_[i]: i for i in list(_.keys())}\n",
    "print(cat_to_name)\n",
    "    \n",
    "# Run this to test the data loader\n",
    "images, labels = next(iter(data_loader['val']))\n",
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226470,
     "status": "ok",
     "timestamp": 1588213233519,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "N350JAHpu8c3",
    "outputId": "96a2d095-f78f-4ca5-eb0c-c5390e367831"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def showimage(data_loader, number_images, cat_to_name):\\n    dataiter = iter(data_loader)\\n    images, labels = dataiter.next()\\n    images = images.numpy() # convert images to numpy for display\\n    # plot the images in the batch, along with the corresponding labels\\n    fig = plt.figure(figsize=(number_images, 4))\\n    for idx in np.arange(number_images):\\n        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\\n        img = np.transpose(images[idx])\\n        plt.imshow(img)\\n        ax.set_title(cat_to_name[labels.tolist()[idx]])\\n        \\n#### to show some  images\\nshowimage(data_loader['test'], 20, cat_to_name)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def showimage(data_loader, number_images, cat_to_name):\n",
    "    dataiter = iter(data_loader)\n",
    "    images, labels = dataiter.next()\n",
    "    images = images.numpy() # convert images to numpy for display\n",
    "    # plot the images in the batch, along with the corresponding labels\n",
    "    fig = plt.figure(figsize=(number_images, 4))\n",
    "    for idx in np.arange(number_images):\n",
    "        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\n",
    "        img = np.transpose(images[idx])\n",
    "        plt.imshow(img)\n",
    "        ax.set_title(cat_to_name[labels.tolist()[idx]])\n",
    "        \n",
    "#### to show some  images\n",
    "showimage(data_loader['test'], 20, cat_to_name)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226461,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "L9jdFtBjSAE6",
    "outputId": "f0f393c5-4369-422c-9aef-fc290ccc941d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1536, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "model = timm.create_model('tf_efficientnet_b3_ap', pretrained=True)\n",
    "#model.fc #show fully connected layer for ResNet family\n",
    "model.classifier #show the classifier layer (fully connected layer) for EfficientNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226454,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "w6QP4CFPBNzg",
    "outputId": "6beb0600-5fdf-4ae6-a216-40c32a13bb9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters of the model is: 14863946\n"
     ]
    }
   ],
   "source": [
    "# Create classifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "# define `classifier` for ResNet\n",
    "# Otherwise, define `fc` for EfficientNet family \n",
    "#because the definition of the full connection/classifier of 2 CNN families is differnt\n",
    "fc = nn.Sequential(OrderedDict([\n",
    "                                 #('fc1', nn.Linear(1536, 1000, bias=True)),\n",
    "                                 ('fc1', nn.Linear(2048, 1000, bias=True)),\n",
    "\t\t\t\t\t\t\t     ('BN1', nn.BatchNorm2d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('dropout1', nn.Dropout(0.7)),\n",
    "                                 ('fc2', nn.Linear(1000, 512)),\n",
    "\t\t\t\t\t\t\t\t ('BN2', nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('swish1', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('dropout2', nn.Dropout(0.5)),\n",
    "\t\t\t\t\t\t\t\t ('fc3', nn.Linear(512, 128)),\n",
    "\t\t\t\t\t\t\t\t ('BN3', nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t     ('swish2', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('fc4', nn.Linear(128, 2)),\n",
    "\t\t\t\t\t\t\t\t ('output', nn.Softmax(dim=1))\n",
    "\t\t\t\t\t\t\t ]))\n",
    "# connect base model (EfficientNet_B0) with modified classifier layer\n",
    "model.fc = fc\n",
    "criterion = LabelSmoothingCrossEntropy()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = Nadam(model.parameters(), lr=0.001)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=0.01,momentum=0.9,\n",
    "                      nesterov=True,\n",
    "                      weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "#lr = lambda x: (((1 + math.cos(x * math.pi / num_epochs)) / 2) ** 1) * 0.9\n",
    "#scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr)\n",
    "#show our model architechture and send to GPU\n",
    "model.to(device)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count = count_parameters(model)\n",
    "print(\"The number of parameters of the model is:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPNx-TodPpVA"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=200, checkpoint = None):\n",
    "    since = time.time()\n",
    "\n",
    "    if checkpoint is None:\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_loss = math.inf\n",
    "        best_acc = 0.\n",
    "    else:\n",
    "        print(f'Val loss: {checkpoint[\"best_val_loss\"]}, Val accuracy: {checkpoint[\"best_val_accuracy\"]}')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best_loss = checkpoint['best_val_loss']\n",
    "        best_acc = checkpoint['best_val_accuracy']\n",
    "   \n",
    "    # Tensorboard summary\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(data_loader[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if i % 1000 == 999:\n",
    "                    print('[%d, %d] loss: %.8f' % \n",
    "                          (epoch + 1, i, running_loss / (i * inputs.size(0))))\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':                \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if phase == 'train':                \n",
    "                scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.8f} Acc: {:.8f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # Record training loss and accuracy for each phase\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('Train/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Train/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "            else:\n",
    "                writer.add_scalar('Valid/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Valid/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "            # deep copy the model\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                print(f'New best model found!')\n",
    "                print(f'New record ACC: {epoch_acc}, previous record acc: {best_acc}')\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save({'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_val_loss': best_loss,\n",
    "                            'best_val_accuracy': best_acc,\n",
    "                            'scheduler_state_dict' : scheduler.state_dict(),\n",
    "                            }, \n",
    "                            CHECK_POINT_PATH\n",
    "                            )\n",
    "                print(f'New record acc is SAVED: {epoch_acc}')\n",
    "\n",
    "        print()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.8f} Best val loss: {:.8f}'.format(best_acc, best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_loss, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "vcXkJFOlP4NJ",
    "outputId": "e47fadb8-c292-4051-8a56-bbdc5868abe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint not found\n",
      "Epoch 0/299\n",
      "----------\n",
      "train Loss: 6.10736631 Acc: 0.07511737\n",
      "val Loss: 3.68139409 Acc: 0.30666667\n",
      "New best model found!\n",
      "New record ACC: 0.3066666666666667, previous record acc: 0.0\n",
      "New record acc is SAVED: 0.3066666666666667\n",
      "\n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: 2.36687404 Acc: 0.57902973\n",
      "val Loss: 1.94740497 Acc: 0.65333333\n",
      "New best model found!\n",
      "New record ACC: 0.6533333333333333, previous record acc: 0.3066666666666667\n",
      "New record acc is SAVED: 0.6533333333333333\n",
      "\n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: 1.87006838 Acc: 0.61815336\n",
      "val Loss: 1.56406963 Acc: 0.78666667\n",
      "New best model found!\n",
      "New record ACC: 0.7866666666666667, previous record acc: 0.6533333333333333\n",
      "New record acc is SAVED: 0.7866666666666667\n",
      "\n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: 1.70673460 Acc: 0.67918623\n",
      "val Loss: 1.61933369 Acc: 0.78666667\n",
      "\n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: 1.61692311 Acc: 0.71205008\n",
      "val Loss: 1.55835922 Acc: 0.74666667\n",
      "\n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: 1.58777993 Acc: 0.70892019\n",
      "val Loss: 1.43862997 Acc: 0.88000000\n",
      "New best model found!\n",
      "New record ACC: 0.88, previous record acc: 0.7866666666666667\n",
      "New record acc is SAVED: 0.88\n",
      "\n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: 1.56637586 Acc: 0.70579030\n",
      "val Loss: 1.41118163 Acc: 0.89333333\n",
      "New best model found!\n",
      "New record ACC: 0.8933333333333334, previous record acc: 0.88\n",
      "New record acc is SAVED: 0.8933333333333334\n",
      "\n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: 1.53097385 Acc: 0.76212833\n",
      "val Loss: 1.40079538 Acc: 0.86666667\n",
      "\n",
      "Epoch 8/299\n",
      "----------\n",
      "train Loss: 1.52128500 Acc: 0.73395931\n",
      "val Loss: 1.35314898 Acc: 0.90666667\n",
      "New best model found!\n",
      "New record ACC: 0.9066666666666667, previous record acc: 0.8933333333333334\n",
      "New record acc is SAVED: 0.9066666666666667\n",
      "\n",
      "Epoch 9/299\n",
      "----------\n",
      "train Loss: 1.43948621 Acc: 0.78560250\n",
      "val Loss: 1.30787519 Acc: 0.92000000\n",
      "New best model found!\n",
      "New record ACC: 0.92, previous record acc: 0.9066666666666667\n",
      "New record acc is SAVED: 0.92\n",
      "\n",
      "Epoch 10/299\n",
      "----------\n",
      "train Loss: 1.49412442 Acc: 0.77464789\n",
      "val Loss: 1.32996745 Acc: 0.85333333\n",
      "\n",
      "Epoch 11/299\n",
      "----------\n",
      "train Loss: 1.49635319 Acc: 0.77308294\n",
      "val Loss: 1.34365679 Acc: 0.86666667\n",
      "\n",
      "Epoch 12/299\n",
      "----------\n",
      "train Loss: 1.47688755 Acc: 0.78247261\n",
      "val Loss: 1.38015660 Acc: 0.85333333\n",
      "\n",
      "Epoch 13/299\n",
      "----------\n",
      "train Loss: 1.47094115 Acc: 0.77621283\n",
      "val Loss: 1.37735728 Acc: 0.81333333\n",
      "\n",
      "Epoch 14/299\n",
      "----------\n",
      "train Loss: 1.46799554 Acc: 0.78873239\n",
      "val Loss: 1.33672248 Acc: 0.88000000\n",
      "\n",
      "Epoch 15/299\n",
      "----------\n",
      "train Loss: 1.43449633 Acc: 0.81064163\n",
      "val Loss: 1.27909190 Acc: 0.90666667\n",
      "\n",
      "Epoch 16/299\n",
      "----------\n",
      "train Loss: 1.40834330 Acc: 0.83881064\n",
      "val Loss: 1.26794516 Acc: 0.89333333\n",
      "\n",
      "Epoch 17/299\n",
      "----------\n",
      "train Loss: 1.38538354 Acc: 0.82003130\n",
      "val Loss: 1.26235828 Acc: 0.92000000\n",
      "\n",
      "Epoch 18/299\n",
      "----------\n",
      "train Loss: 1.40015552 Acc: 0.81846635\n",
      "val Loss: 1.26958329 Acc: 0.90666667\n",
      "\n",
      "Epoch 19/299\n",
      "----------\n",
      "train Loss: 1.41329200 Acc: 0.81533646\n",
      "val Loss: 1.31437891 Acc: 0.85333333\n",
      "\n",
      "Epoch 20/299\n",
      "----------\n",
      "train Loss: 1.41146777 Acc: 0.80751174\n",
      "val Loss: 1.24380956 Acc: 0.94666667\n",
      "New best model found!\n",
      "New record ACC: 0.9466666666666668, previous record acc: 0.92\n",
      "New record acc is SAVED: 0.9466666666666668\n",
      "\n",
      "Epoch 21/299\n",
      "----------\n",
      "train Loss: 1.36844140 Acc: 0.84663537\n",
      "val Loss: 1.24709044 Acc: 0.90666667\n",
      "\n",
      "Epoch 22/299\n",
      "----------\n",
      "train Loss: 1.36014337 Acc: 0.84350548\n",
      "val Loss: 1.22120820 Acc: 0.96000000\n",
      "New best model found!\n",
      "New record ACC: 0.9600000000000001, previous record acc: 0.9466666666666668\n",
      "New record acc is SAVED: 0.9600000000000001\n",
      "\n",
      "Epoch 23/299\n",
      "----------\n",
      "train Loss: 1.34639912 Acc: 0.84350548\n",
      "val Loss: 1.22856672 Acc: 0.93333333\n",
      "\n",
      "Epoch 24/299\n",
      "----------\n",
      "train Loss: 1.35160802 Acc: 0.84194053\n",
      "val Loss: 1.25497661 Acc: 0.92000000\n",
      "\n",
      "Epoch 25/299\n",
      "----------\n",
      "train Loss: 1.37439108 Acc: 0.83098592\n",
      "val Loss: 1.27484357 Acc: 0.88000000\n",
      "\n",
      "Epoch 26/299\n",
      "----------\n",
      "train Loss: 1.36044499 Acc: 0.84663537\n",
      "val Loss: 1.24784958 Acc: 0.90666667\n",
      "\n",
      "Epoch 27/299\n",
      "----------\n",
      "train Loss: 1.34294514 Acc: 0.85758998\n",
      "val Loss: 1.27409345 Acc: 0.92000000\n",
      "\n",
      "Epoch 28/299\n",
      "----------\n",
      "train Loss: 1.33772311 Acc: 0.85446009\n",
      "val Loss: 1.26801396 Acc: 0.90666667\n",
      "\n",
      "Epoch 29/299\n",
      "----------\n",
      "train Loss: 1.33876391 Acc: 0.85915493\n",
      "val Loss: 1.26643965 Acc: 0.89333333\n",
      "\n",
      "Epoch 30/299\n",
      "----------\n",
      "train Loss: 1.31662320 Acc: 0.87167449\n",
      "val Loss: 1.23431834 Acc: 0.90666667\n",
      "\n",
      "Epoch 31/299\n",
      "----------\n",
      "train Loss: 1.28706095 Acc: 0.89827856\n",
      "val Loss: 1.23810408 Acc: 0.92000000\n",
      "\n",
      "Epoch 32/299\n",
      "----------\n",
      "train Loss: 1.27864800 Acc: 0.88732394\n",
      "val Loss: 1.20120896 Acc: 0.94666667\n",
      "\n",
      "Epoch 33/299\n",
      "----------\n",
      "train Loss: 1.34084850 Acc: 0.85915493\n",
      "val Loss: 1.24502130 Acc: 0.93333333\n",
      "\n",
      "Epoch 34/299\n",
      "----------\n",
      "train Loss: 1.32616028 Acc: 0.84350548\n",
      "val Loss: 1.22378000 Acc: 0.92000000\n",
      "\n",
      "Epoch 35/299\n",
      "----------\n",
      "train Loss: 1.28253345 Acc: 0.88106416\n",
      "val Loss: 1.20369836 Acc: 0.94666667\n",
      "\n",
      "Epoch 36/299\n",
      "----------\n",
      "train Loss: 1.31424544 Acc: 0.86384977\n",
      "val Loss: 1.20729575 Acc: 0.92000000\n",
      "\n",
      "Epoch 37/299\n",
      "----------\n",
      "train Loss: 1.29878555 Acc: 0.88419405\n",
      "val Loss: 1.19397343 Acc: 0.94666667\n",
      "\n",
      "Epoch 38/299\n",
      "----------\n",
      "train Loss: 1.28645044 Acc: 0.88575900\n",
      "val Loss: 1.25379276 Acc: 0.86666667\n",
      "\n",
      "Epoch 39/299\n",
      "----------\n",
      "train Loss: 1.28299272 Acc: 0.88888889\n",
      "val Loss: 1.16577055 Acc: 0.97333333\n",
      "New best model found!\n",
      "New record ACC: 0.9733333333333334, previous record acc: 0.9600000000000001\n",
      "New record acc is SAVED: 0.9733333333333334\n",
      "\n",
      "Epoch 40/299\n",
      "----------\n",
      "train Loss: 1.28560810 Acc: 0.87636933\n",
      "val Loss: 1.15546977 Acc: 0.93333333\n",
      "\n",
      "Epoch 41/299\n",
      "----------\n",
      "train Loss: 1.23816258 Acc: 0.90140845\n",
      "val Loss: 1.23210717 Acc: 0.89333333\n",
      "\n",
      "Epoch 42/299\n",
      "----------\n",
      "train Loss: 1.28219813 Acc: 0.88732394\n",
      "val Loss: 1.19818951 Acc: 0.92000000\n",
      "\n",
      "Epoch 43/299\n",
      "----------\n",
      "train Loss: 1.28281070 Acc: 0.86854460\n",
      "val Loss: 1.15978049 Acc: 0.96000000\n",
      "\n",
      "Epoch 44/299\n",
      "----------\n",
      "train Loss: 1.25460474 Acc: 0.89514867\n",
      "val Loss: 1.14721464 Acc: 0.94666667\n",
      "\n",
      "Epoch 45/299\n",
      "----------\n",
      "train Loss: 1.28489299 Acc: 0.86384977\n",
      "val Loss: 1.20546803 Acc: 0.90666667\n",
      "\n",
      "Epoch 46/299\n",
      "----------\n",
      "train Loss: 1.28228680 Acc: 0.86541471\n",
      "val Loss: 1.17068837 Acc: 0.97333333\n",
      "\n",
      "Epoch 47/299\n",
      "----------\n",
      "train Loss: 1.27560293 Acc: 0.89045383\n",
      "val Loss: 1.19466160 Acc: 0.93333333\n",
      "\n",
      "Epoch 48/299\n",
      "----------\n",
      "train Loss: 1.26908951 Acc: 0.89671362\n",
      "val Loss: 1.19746604 Acc: 0.93333333\n",
      "\n",
      "Epoch 49/299\n",
      "----------\n",
      "train Loss: 1.23325475 Acc: 0.91079812\n",
      "val Loss: 1.14262261 Acc: 0.97333333\n",
      "\n",
      "Epoch 50/299\n",
      "----------\n",
      "train Loss: 1.25366120 Acc: 0.90297340\n",
      "val Loss: 1.12200021 Acc: 0.96000000\n",
      "\n",
      "Epoch 51/299\n",
      "----------\n",
      "train Loss: 1.25353965 Acc: 0.89984351\n",
      "val Loss: 1.19098813 Acc: 0.93333333\n",
      "\n",
      "Epoch 52/299\n",
      "----------\n",
      "train Loss: 1.24979550 Acc: 0.89514867\n",
      "val Loss: 1.12914930 Acc: 0.97333333\n",
      "\n",
      "Epoch 53/299\n",
      "----------\n",
      "train Loss: 1.26854501 Acc: 0.87793427\n",
      "val Loss: 1.09962330 Acc: 1.00000000\n",
      "New best model found!\n",
      "New record ACC: 1.0, previous record acc: 0.9733333333333334\n",
      "New record acc is SAVED: 1.0\n",
      "\n",
      "Epoch 54/299\n",
      "----------\n",
      "train Loss: 1.24528670 Acc: 0.90923318\n",
      "val Loss: 1.11755316 Acc: 0.97333333\n",
      "\n",
      "Epoch 55/299\n",
      "----------\n",
      "train Loss: 1.21200862 Acc: 0.92018779\n",
      "val Loss: 1.16017148 Acc: 0.96000000\n",
      "\n",
      "Epoch 56/299\n",
      "----------\n",
      "train Loss: 1.24245151 Acc: 0.89827856\n",
      "val Loss: 1.20326231 Acc: 0.93333333\n",
      "\n",
      "Epoch 57/299\n",
      "----------\n",
      "train Loss: 1.28375555 Acc: 0.86854460\n",
      "val Loss: 1.18319183 Acc: 0.92000000\n",
      "\n",
      "Epoch 58/299\n",
      "----------\n",
      "train Loss: 1.20682656 Acc: 0.92018779\n",
      "val Loss: 1.16590704 Acc: 0.94666667\n",
      "\n",
      "Epoch 59/299\n",
      "----------\n",
      "train Loss: 1.24681118 Acc: 0.90140845\n",
      "val Loss: 1.30593357 Acc: 0.85333333\n",
      "\n",
      "Epoch 60/299\n",
      "----------\n",
      "train Loss: 1.24173780 Acc: 0.89984351\n",
      "val Loss: 1.35295971 Acc: 0.78666667\n",
      "\n",
      "Epoch 61/299\n",
      "----------\n",
      "train Loss: 1.22183647 Acc: 0.91862285\n",
      "val Loss: 1.25723160 Acc: 0.90666667\n",
      "\n",
      "Epoch 62/299\n",
      "----------\n",
      "train Loss: 1.23655340 Acc: 0.90766823\n",
      "val Loss: 1.20938404 Acc: 0.92000000\n",
      "\n",
      "Epoch 63/299\n",
      "----------\n",
      "train Loss: 1.24712651 Acc: 0.89358372\n",
      "val Loss: 1.14500728 Acc: 0.96000000\n",
      "\n",
      "Epoch 64/299\n",
      "----------\n",
      "train Loss: 1.24718357 Acc: 0.89827856\n",
      "val Loss: 1.15613039 Acc: 0.93333333\n",
      "\n",
      "Epoch 65/299\n",
      "----------\n",
      "train Loss: 1.22463743 Acc: 0.90453834\n",
      "val Loss: 1.14180134 Acc: 0.96000000\n",
      "\n",
      "Epoch 66/299\n",
      "----------\n",
      "train Loss: 1.22750820 Acc: 0.91079812\n",
      "val Loss: 1.10545498 Acc: 0.94666667\n",
      "\n",
      "Epoch 67/299\n",
      "----------\n",
      "train Loss: 1.21974209 Acc: 0.91705790\n",
      "val Loss: 1.15773308 Acc: 0.93333333\n",
      "\n",
      "Epoch 68/299\n",
      "----------\n",
      "train Loss: 1.22267871 Acc: 0.90610329\n",
      "val Loss: 1.19219435 Acc: 0.92000000\n",
      "\n",
      "Epoch 69/299\n",
      "----------\n",
      "train Loss: 1.23565809 Acc: 0.90453834\n",
      "val Loss: 1.13904350 Acc: 0.97333333\n",
      "\n",
      "Epoch 70/299\n",
      "----------\n",
      "train Loss: 1.20095022 Acc: 0.92644757\n",
      "val Loss: 1.12575944 Acc: 0.94666667\n",
      "\n",
      "Epoch 71/299\n",
      "----------\n",
      "train Loss: 1.22389978 Acc: 0.90923318\n",
      "val Loss: 1.14766276 Acc: 0.96000000\n",
      "\n",
      "Epoch 72/299\n",
      "----------\n",
      "train Loss: 1.18904339 Acc: 0.93740219\n",
      "val Loss: 1.15610829 Acc: 0.92000000\n",
      "\n",
      "Epoch 73/299\n",
      "----------\n",
      "train Loss: 1.21102230 Acc: 0.92644757\n",
      "val Loss: 1.20654376 Acc: 0.92000000\n",
      "\n",
      "Epoch 74/299\n",
      "----------\n",
      "train Loss: 1.19529430 Acc: 0.92018779\n",
      "val Loss: 1.18333835 Acc: 0.92000000\n",
      "\n",
      "Epoch 75/299\n",
      "----------\n",
      "train Loss: 1.22785334 Acc: 0.89827856\n",
      "val Loss: 1.20451934 Acc: 0.93333333\n",
      "\n",
      "Epoch 76/299\n",
      "----------\n",
      "train Loss: 1.23681315 Acc: 0.89827856\n",
      "val Loss: 1.19696333 Acc: 0.94666667\n",
      "\n",
      "Epoch 77/299\n",
      "----------\n",
      "train Loss: 1.17210044 Acc: 0.94366197\n",
      "val Loss: 1.17342697 Acc: 0.92000000\n",
      "\n",
      "Epoch 78/299\n",
      "----------\n",
      "train Loss: 1.19692049 Acc: 0.91862285\n",
      "val Loss: 1.17373716 Acc: 0.94666667\n",
      "\n",
      "Epoch 79/299\n",
      "----------\n",
      "train Loss: 1.17088153 Acc: 0.92488263\n",
      "val Loss: 1.13652164 Acc: 0.96000000\n",
      "\n",
      "Epoch 80/299\n",
      "----------\n",
      "train Loss: 1.21739883 Acc: 0.91862285\n",
      "val Loss: 1.18163823 Acc: 0.93333333\n",
      "\n",
      "Epoch 81/299\n",
      "----------\n",
      "train Loss: 1.21816704 Acc: 0.90140845\n",
      "val Loss: 1.20156152 Acc: 0.92000000\n",
      "\n",
      "Epoch 82/299\n",
      "----------\n",
      "train Loss: 1.21789103 Acc: 0.90923318\n",
      "val Loss: 1.15879605 Acc: 0.92000000\n",
      "\n",
      "Epoch 83/299\n",
      "----------\n",
      "train Loss: 1.21578297 Acc: 0.90766823\n",
      "val Loss: 1.20980685 Acc: 0.92000000\n",
      "\n",
      "Epoch 84/299\n",
      "----------\n",
      "train Loss: 1.18320050 Acc: 0.93114241\n",
      "val Loss: 1.24986887 Acc: 0.92000000\n",
      "\n",
      "Epoch 85/299\n",
      "----------\n",
      "train Loss: 1.17102397 Acc: 0.93427230\n",
      "val Loss: 1.21285952 Acc: 0.90666667\n",
      "\n",
      "Epoch 86/299\n",
      "----------\n",
      "train Loss: 1.18629365 Acc: 0.92175274\n",
      "val Loss: 1.14683536 Acc: 0.93333333\n",
      "\n",
      "Epoch 87/299\n",
      "----------\n",
      "train Loss: 1.15768673 Acc: 0.94679186\n",
      "val Loss: 1.14384448 Acc: 0.94666667\n",
      "\n",
      "Epoch 88/299\n",
      "----------\n",
      "train Loss: 1.18003446 Acc: 0.93270736\n",
      "val Loss: 1.15880687 Acc: 0.94666667\n",
      "\n",
      "Epoch 89/299\n",
      "----------\n",
      "train Loss: 1.16646094 Acc: 0.91705790\n",
      "val Loss: 1.10222372 Acc: 1.00000000\n",
      "\n",
      "Epoch 90/299\n",
      "----------\n",
      "train Loss: 1.17362846 Acc: 0.93740219\n",
      "val Loss: 1.11496141 Acc: 0.97333333\n",
      "\n",
      "Epoch 91/299\n",
      "----------\n",
      "train Loss: 1.20206206 Acc: 0.92018779\n",
      "val Loss: 1.11861253 Acc: 0.96000000\n",
      "\n",
      "Epoch 92/299\n",
      "----------\n",
      "train Loss: 1.18100046 Acc: 0.92488263\n",
      "val Loss: 1.11362793 Acc: 0.97333333\n",
      "\n",
      "Epoch 93/299\n",
      "----------\n",
      "train Loss: 1.19143937 Acc: 0.92801252\n",
      "val Loss: 1.15224005 Acc: 0.96000000\n",
      "\n",
      "Epoch 94/299\n",
      "----------\n",
      "train Loss: 1.17237472 Acc: 0.92644757\n",
      "val Loss: 1.16609949 Acc: 0.96000000\n",
      "\n",
      "Epoch 95/299\n",
      "----------\n",
      "train Loss: 1.19654442 Acc: 0.92644757\n",
      "val Loss: 1.18339567 Acc: 0.96000000\n",
      "\n",
      "Epoch 96/299\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "CHECK_POINT_PATH = '/home/linh/Downloads/Covid-19/weights_CT/EfficientNet_B3_AP_dataset_Yang.pth'\n",
    "#CHECK_POINT_PATH = '/home/linh/Downloads/Covid-19/weights_CT/EfficientNet_B3_AP_dataset_Moh.pth'\n",
    "#CHECK_POINT_PATH = '/home/linh/Downloads/Covid-19/weights_CT/EfficientNet_B3_AP_dataset_Soares.pth'\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "    print(\"checkpoint loaded\")\n",
    "except:\n",
    "    checkpoint = None\n",
    "    print(\"checkpoint not found\")\n",
    "if checkpoint == None:\n",
    "    CHECK_POINT_PATH = CHECK_POINT_PATH\n",
    "model, best_val_loss, best_val_acc = train_model(model,\n",
    "                                                 criterion,\n",
    "                                                 optimizer,\n",
    "                                                 scheduler,\n",
    "                                                 num_epochs = num_epochs,\n",
    "                                                 checkpoint = None# torch.load(CHECK_POINT_PATH)\n",
    "                                                 ) \n",
    "                                                \n",
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_val_accuracy': best_val_acc,\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            }, CHECK_POINT_PATH)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Covid-19_EfficientNet_B0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
